{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gj42j8sIhWL_"
      },
      "outputs": [],
      "source": [
        "# Upload audio file\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# path = next(iter(uploaded))\n",
        "\n",
        "path = \"/content/audio.wav\"\n",
        "num_speakers = 4  # You can adjust the number of speakers here\n",
        "\n",
        "language = 'English'  # You can specify the language here\n",
        "\n",
        "model_size = 'tiny'  # You can choose the model size here\n",
        "\n",
        "model_name = model_size\n",
        "if language == 'English' and model_size != 'large':\n",
        "    model_name += '.en'\n",
        "\n",
        "# Install required packages\n",
        "#!pip install -q git+https://github.com/openai/whisper.git > /dev/null\n",
        "#!pip install -q git+https://github.com/pyannote/pyannote-audio > /dev/null\n",
        "\n",
        "import whisper\n",
        "import subprocess\n",
        "import torch\n",
        "import pyannote.audio\n",
        "from pyannote.audio.pipelines.speaker_verification import PretrainedSpeakerEmbedding\n",
        "from pyannote.audio import Audio\n",
        "from pyannote.core import Segment\n",
        "import wave\n",
        "import contextlib\n",
        "import random\n",
        "from pydub import AudioSegment\n",
        "import pyaudio\n",
        "import os\n",
        "from datetime import timedelta\n",
        "import requests\n",
        "from transformers import pipeline\n",
        "import numpy as np\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "\n",
        "# Define a function to generate a random name\n",
        "def generate_name():\n",
        "    first_names = [\"Emma\", \"Liam\", \"Ava\", \"Noah\", \"Sophia\", \"William\", \"Isabella\", \"James\", \"Mia\", \"Benjamin\", \"Charlotte\", \"Oliver\", \"Amelia\", \"Evelyn\", \"Henry\", \"Harper\", \"Ella\", \"Alexander\", \"Abigail\", \"Michael\"]\n",
        "    last_names = [\"Smith\", \"Johnson\", \"Brown\", \"Garcia\", \"Miller\", \"Davis\", \"Rodriguez\", \"Martinez\", \"Hernandez\", \"Lopez\", \"Gonzalez\", \"Perez\", \"Taylor\", \"Anderson\", \"Wilson\", \"Jackson\", \"White\", \"Harris\", \"Martin\", \"Thompson\"]\n",
        "    first_name = random.choice(first_names)\n",
        "    last_name = random.choice(last_names)\n",
        "    return f\"{first_name} {last_name}\"\n",
        "\n",
        "def speaker_diarization(path, num_speakers, language, model_size):\n",
        "    # Define the embedding model\n",
        "    embedding_model = PretrainedSpeakerEmbedding(\"speechbrain/spkrec-ecapa-voxceleb\",device=torch.device(\"cuda\"))\n",
        "\n",
        "    # Check if the input path is not a WAV file, convert it to WAV\n",
        "    if path[-3:] != 'wav':\n",
        "        subprocess.call(['ffmpeg', '-i', path, 'audio.wav', '-y'])\n",
        "        path = 'audio.wav'\n",
        "\n",
        "    # Load the Whisper model\n",
        "    model = whisper.load_model(model_size)\n",
        "\n",
        "    # Transcribe the audio using Whisper\n",
        "    result = model.transcribe(path)\n",
        "    segments = result[\"segments\"]\n",
        "\n",
        "    # Extract audio duration\n",
        "    with contextlib.closing(wave.open(path, 'r')) as f:\n",
        "        frames = f.getnframes()\n",
        "        rate = f.getframerate()\n",
        "        duration = frames / float(rate)\n",
        "\n",
        "    audio = Audio()\n",
        "\n",
        "    def segment_embedding(segment):\n",
        "        start = segment[\"start\"]\n",
        "        end = min(duration, segment[\"end\"])\n",
        "        clip = Segment(start, end)\n",
        "        waveform, samplerate = audio.crop(path, clip)\n",
        "        return embedding_model(waveform[None])\n",
        "\n",
        "    embeddings = np.zeros(shape=(len(segments), 192))\n",
        "    for i, segment in enumerate(segments):\n",
        "        embeddings[i] = segment_embedding(segment)\n",
        "\n",
        "    embeddings = np.nan_to_num(embeddings)\n",
        "\n",
        "    # Perform speaker clustering\n",
        "    clustering = AgglomerativeClustering(num_speakers).fit(embeddings)\n",
        "    labels = clustering.labels_\n",
        "    for i in range(len(segments)):\n",
        "        segments[i][\"speaker\"] = 'SPEAKER ' + str(labels[i] + 1)\n",
        "\n",
        "    def time(secs):\n",
        "        return timedelta(seconds=round(secs))\n",
        "\n",
        "    # Generate transcript\n",
        "    with open(\"transcript.txt\", \"w\", encoding='utf-8') as f:\n",
        "        speakers = []\n",
        "        distinct_speakers = []\n",
        "        for (i, segment) in enumerate(segments):\n",
        "            if i == 0 or segments[i - 1][\"speaker\"] != segment[\"speaker\"]:\n",
        "                f.write(\"\\n\" + segment[\"speaker\"] + ' ' + str(time(segment[\"start\"])) + '\\n')\n",
        "            f.write(segment[\"text\"][1:] + ' ')\n",
        "            temp = set(speakers)\n",
        "            for x in temp:\n",
        "                if x not in distinct_speakers:\n",
        "                    distinct_speakers.append(segment[\"speaker\"])\n",
        "                    distinct_speakers.append(segment[\"start\"])\n",
        "                    distinct_speakers.append(segment[\"end\"])\n",
        "\n",
        "    # Process speaker names and audio segments\n",
        "    speaker_names = []\n",
        "    val = len(distinct_speakers) // 3\n",
        "    for i in range(val):\n",
        "        j = 1 + i * 3\n",
        "        start_time_str = str(distinct_speakers[j])\n",
        "        end_time_str = str(distinct_speakers[j + 1])\n",
        "        start_time = float(start_time_str)\n",
        "        end_time = float(end_time_str)\n",
        "\n",
        "        # Load the audio file as an AudioSegment\n",
        "        audio_segment = AudioSegment.from_wav(path)\n",
        "\n",
        "        # Extract the desired segment\n",
        "        segment = audio_segment[int(start_time * 1000):int(end_time * 1000)]\n",
        "\n",
        "        # Export the segment to an MP3 file\n",
        "        name = generate_name()\n",
        "        speaker_names.append(name)\n",
        "        export_file = name + \".mp3\"\n",
        "        segment.export(export_file, format=\"mp3\")\n",
        "\n",
        "    i = 0\n",
        "    dialogues = {}\n",
        "\n",
        "    for (i, segment) in enumerate(segments):\n",
        "        if i == 0 or segments[i - 1][\"speaker\"] != segment[\"speaker\"]:\n",
        "            temp1 = time(segment[\"start\"])\n",
        "            temp2 = time(segment[\"end\"])\n",
        "            # Initialize a dialogue list for this speaker\n",
        "            dialogues[segment[\"speaker\"]] = []\n",
        "\n",
        "        dialogues[segment[\"speaker\"]].append(segment[\"text\"][1:])\n",
        "\n",
        "    # Save dialogues for each speaker to separate files\n",
        "    for speaker, speaker_dialogues in dialogues.items():\n",
        "        with open(f\"{speaker}.txt\", \"w\", encoding='utf-8') as f:\n",
        "            f.write(\"\\n\".join(speaker_dialogues))\n",
        "\n",
        "    # Load text for summarization\n",
        "    with open(\"transcript.txt\", \"r\", encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    # Summarize the text\n",
        "    # summarizer = pipeline(\"summarization\", model=\"knkarthick/MEETING-SUMMARY-BART-LARGE-XSUM-SAMSUM-DIALOGSUM-AMI\")\n",
        "    # summary = summarizer(text)\n",
        "    hf_name = 'pszemraj/led-large-book-summary'\n",
        "    summarizer = pipeline(\"summarization\", hf_name,device=0 if torch.cuda.is_available() else -1,encoder_no_repeat_ngram_size=3 )\n",
        "    with open(\"transcript.txt\", \"r\", encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    summary = summarizer(text, max_length=150)  # You can adjust max_length as needed\n",
        "\n",
        "    # Save the summary\n",
        "    with open(\"summary.txt\", \"w\", encoding='utf-8') as f:\n",
        "        f.write(summary[0][\"summary_text\"])\n",
        "\n",
        "    # Save the summary\n",
        "    with open(\"summary.txt\", \"w\", encoding='utf-8') as f:\n",
        "        for sentence in summary:\n",
        "            f.write(str(sentence))\n",
        "\n",
        "    # Process summaries for each speaker\n",
        "    for speaker_name in speaker_names:\n",
        "        if os.path.exists(f\"{speaker_name}.txt\"):\n",
        "            with open(f\"{speaker_name}.txt\", \"r\", encoding='utf-8') as f:\n",
        "                text = f.read()\n",
        "                summary = summarizer(text)\n",
        "                with open(f\"{speaker_name}_summary.txt\", \"w\", encoding='utf-8') as f:\n",
        "                    for sentence in summary:\n",
        "                        f.write(str(sentence))\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    # One AI API call (provide your API key)\n",
        "    api_key = \"f4e72987-82dd-44d4-8d57-bea926956674\"\n",
        "    url = \"https://api.oneai.com/api/v0/pipeline\"\n",
        "    headers = {\n",
        "        \"api-key\": api_key,\n",
        "        \"content-type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    # Process the main transcript using One AI\n",
        "    with open(\"transcript.txt\", \"r\", encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    payload = {\n",
        "        \"input\": text,\n",
        "        \"input_type\": \"article\",\n",
        "        \"output_type\": \"json\",\n",
        "        \"multilingual\": {\n",
        "            \"enabled\": True\n",
        "        },\n",
        "        \"steps\": [\n",
        "            {\n",
        "                \"skill\": \"action-items\"\n",
        "            }\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    r = requests.post(url, json=payload, headers=headers)\n",
        "    data = r.json()\n",
        "    input_dict = data\n",
        "    labels = input_dict['output'][0]['labels']\n",
        "    action_items = [label['value'] for label in labels if label['type'] == 'action-item']\n",
        "\n",
        "    with open(\"actions.txt\", \"w\") as f:\n",
        "        f.write(str(action_items))\n",
        "\n",
        "    # Process summaries for each speaker using One AI\n",
        "    for speaker_name in speaker_names:\n",
        "        with open(f\"{speaker_name}_summary.txt\", \"r\", encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "            payload = {\n",
        "                \"input\": text,\n",
        "                \"input_type\": \"article\",\n",
        "                \"output_type\": \"json\",\n",
        "                \"multilingual\": {\n",
        "                    \"enabled\": True\n",
        "                },\n",
        "                \"steps\": [\n",
        "                    {\n",
        "                        \"skill\": \"action-items\"\n",
        "                    }\n",
        "                ],\n",
        "            }\n",
        "\n",
        "            r = requests.post(url, json=payload, headers=headers)\n",
        "            data = r.json()\n",
        "            input_dict = data\n",
        "            labels = input_dict['output'][0]['labels']\n",
        "            action_items = [label['value'] for label in labels if label['type'] == 'action-item']\n",
        "\n",
        "        with open(f\"{speaker_name}_actions.txt\", \"w\", encoding='utf-8') as f:\n",
        "            f.write(str(action_items))\n",
        "\n",
        "# Example usage:\n",
        "speaker_diarization(path, num_speakers, language, model_size)\n"
      ]
    }
  ]
}